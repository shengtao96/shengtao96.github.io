<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Kernel Principal Component Analysis | 鲭兜的博客 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="shengtao96">
    
    

    <meta name="description" content="主成分分析（Principal Component Analysis）是降维（Demension Reduction）的重要手段。但是就像上篇文章所说，使用主成分分析也存在一些限制，它可以很好地解除特征量的线性相关性问题，但是对于高阶相关性就没有办法了（协方差等于0，意味着特征量之间不存在线性相关性，而不是完全独立，可能仍然存在高阶相关性）。这个时候，我们考虑Kernel PCA，通过Kernel">
<meta property="og:type" content="article">
<meta property="og:title" content="Kernel Principal Component Analysis | 鲭兜的博客">
<meta property="og:url" content="http://shengtao96.github.io/2017/06/09/Kernel-Principal-Component-Analysis/index.html">
<meta property="og:site_name" content="鲭兜的博客">
<meta property="og:description" content="主成分分析（Principal Component Analysis）是降维（Demension Reduction）的重要手段。但是就像上篇文章所说，使用主成分分析也存在一些限制，它可以很好地解除特征量的线性相关性问题，但是对于高阶相关性就没有办法了（协方差等于0，意味着特征量之间不存在线性相关性，而不是完全独立，可能仍然存在高阶相关性）。这个时候，我们考虑Kernel PCA，通过Kernel">
<meta property="og:updated_time" content="2017-06-09T08:48:08.524Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kernel Principal Component Analysis | 鲭兜的博客">
<meta name="twitter:description" content="主成分分析（Principal Component Analysis）是降维（Demension Reduction）的重要手段。但是就像上篇文章所说，使用主成分分析也存在一些限制，它可以很好地解除特征量的线性相关性问题，但是对于高阶相关性就没有办法了（协方差等于0，意味着特征量之间不存在线性相关性，而不是完全独立，可能仍然存在高阶相关性）。这个时候，我们考虑Kernel PCA，通过Kernel">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">鲭兜的博客</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          努力に胜る天才无し
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/shengtao96" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Kernel Principal Component Analysis</h1>

    

    <div class="post-meta">
      <time datetime="2017-06-09" class="post-meta__date date">2017-06-09</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/程序猿之路净化一切/">程序猿之路净化一切</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/数学/">数学</a>, <a class="tags-link" href="/tags/机器学习/">机器学习</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=408522477&auto=1&height=66"></iframe><br>主成分分析（Principal Component Analysis）是降维（Demension Reduction）的重要手段。但是就像上篇文章所说，使用主成分分析也存在一些限制，它可以很好地解除特征量的线性相关性问题，但是对于高阶相关性就没有办法了（协方差等于0，意味着特征量之间不存在线性相关性，而不是完全独立，可能仍然存在高阶相关性）。这个时候，我们考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，从而进行PCA转换。<br>这篇文章的目的是记录我学习Kernel PCA过程的心得体会，简单介绍一下Kernel PCA的数学原理，文章有错误的地方欢迎指正。</p>
<h2 id="0、几个假设"><a href="#0、几个假设" class="headerlink" title="0、几个假设"></a>0、几个假设</h2><p>对于PCA，实际上存在几个假设：<br>（1）特征值的大小决定了该字段的信息多少，即小特征值往往表示噪声。但实际上，向小特征值方向投影也有可能包括许多信息。<br>（2）特征向量的方向是互相正交（orthogonal）的，这种正交性使得PCA容易受到离群点（Outlier）的影响。<br>（3）难于解释结果。例如在建立线性回归模型（Linear Regression Model）分析因变量（responce）和第一个主成分的关系时，我们得到的回归系数（coefficiency）不是某一个自变量（covariate）的贡献，而是所有自变量的某个线性组合（Linear Combination）的贡献。</p>
<p>在Kernel PCA中，我们同样需要这些假设，但不同的地方是我们认为原始数据有更高的维度，我们可以在更高维的空间（Hilbert Space）中做PCA分析（即在更高维空间里，把原始数据向不同的方向投影）。这样做，对于在通常线性空间难以线性分类的数据点，我们有可能在更高维度上找到合适的高维线性分类平面。</p>
<h2 id="1、Kernel-Principal-Component-Analysis"><a href="#1、Kernel-Principal-Component-Analysis" class="headerlink" title="1、Kernel Principal Component Analysis"></a>1、Kernel Principal Component Analysis</h2><p>我们对于传统PCA算法，类比出在高维空间的PCA算法。</p>
<h3 id="1-1、传统PCA的做法"><a href="#1-1、传统PCA的做法" class="headerlink" title="1-1、传统PCA的做法"></a>1-1、传统PCA的做法</h3><p>我们先定义必要的变量和需要解决的问题：$X=[\begin&#123;matrix&#125; x_1 &amp; x_2 &amp; \dots &amp; x_N]\end&#123;matrix&#125;$是一个$d \times N$的矩阵，代表输入的数据有$N$个，每个样本的维度是$d$。我们做降维，就是想用$k$维的数据来表示原始的$d$维数据（$k &lt; d$）。<br>我们首先将每个字段的均值化为0，即$\sum_ix_i=0$，定义协方差矩阵$C$为：<br>$$C=\frac&#123;1&#125;&#123;N&#125;XX^T$$<br>做特征值分解，我们得到：<br>$$CU=U\Lambda\Rightarrow C=U\Lambda U^T=\displaystyle\sum_a\lambda_au_a&#123;u_a&#125;^T$$<br>注意这里的$C,U,\Lambda$的维度都是$d\times d$，且$U=[\begin&#123;matrix&#125; u_1 &amp; u_2 &amp; \dots &amp; u_d]\end&#123;matrix&#125;$，$\Lambda=diag(\begin&#123;matrix&#125; \lambda_1 &amp; \lambda_2 &amp; \dots &amp; \lambda_d \end&#123;matrix&#125;)$。当我们做降维时，可以利用前$k$个特征向量$U_k=[\begin&#123;matrix&#125; u_1 &amp; u_2 &amp; \dots &amp; u_k \end&#123;matrix&#125;]$，则将一个$d$维的$x_i$向$k$维的主成分方向投影后的数据为$y_i=U_k^Tx_i$。</p>
<h3 id="1-2、高维空间中的PCA"><a href="#1-2、高维空间中的PCA" class="headerlink" title="1-2、高维空间中的PCA"></a>1-2、高维空间中的PCA</h3><p>高维空间中，我们定义一个映射$\Phi:X^d\rightarrow\mathcal&#123;F&#125;$，这里$\mathcal&#123;F&#125;$表示Hilbert泛函空间。<br>现在我们的输入数据是$\Phi(x_i),i=1,2,\dots,n$，他们的维度可以是无穷维的（泛函空间），为了方便讨论，设其空间的维度为$D（D\gg d）$。<br>在这个新的空间（特征空间）中，将中心化的数据记为$\tilde\Phi(x_i)$，即<br>$$\tilde\Phi(x_i)=\Phi(x_i)-\bar\Phi$$<br>其中<br>$$\bar\Phi=\frac&#123;1&#125;&#123;N&#125;\sum_i^N\Phi(x_i)$$<br>将中心化的核矩阵记为$\tilde K$（核矩阵是样本之间通过核函数映射之后得到的，每两个样本之间进行一次核函数映射），且定义$\tilde K_&#123;ij&#125;\le \tilde\Phi(x_i)$，$\tilde\Phi(x_j)\ge\tilde\Phi(x_i)^T\tilde\Phi(x_j)$。<br>$\tilde\Phi(x_i)$中心化的数据（$\displaystyle\sum_i^N\tilde\Phi(x_i)=0$）的协方差矩阵为$\bar&#123;C&#125;$：<br>$$\bar C=\frac&#123;1&#125;&#123;N&#125;\tilde\Phi(X)\tilde\Phi(X)^T$$<br><strong>这里有一个陷阱：</strong><br>对Kernel Trick一知半解的时候，我们常常从形式上认为$\bar C$可以用$\tilde K_&#123;ij&#125;=\tilde K(x_i,x_j)$来代替，因此对$\tilde K=(\tilde K_&#123;ij&#125;)$做特征值分解，然后得到$\tilde K=U\Lambda U^T$，并且对原始数据降维的时候，定义$Y_i=U_k^TX_i$。<br>但这个错误的方法有两个问题：一是我们不知道矩阵$\bar C$的维度，即也不知道$\bar C$的具体形式；二是$U_k^TX_i$从形式上看不出是从高维空间的$\Phi(x_i)$的投影，并且当有新的数据$X_&#123;new&#125;$时，我们无法从理论上理解$U_k^TX_&#123;new&#125;$是从高维空间的投影，后面发现，投影方向不是这里的$U$。<br>如果应用这种错误的方法，我们有可能得到看起来差不多正确的结果，但本质上这是错误的。正确的方法是通过Kernel Trick将PCA投影的过程通过内积的形式表现出来。</p>
<h3 id="1-3、利用Kernel-Trick在高维空间做PCA"><a href="#1-3、利用Kernel-Trick在高维空间做PCA" class="headerlink" title="1-3、利用Kernel Trick在高维空间做PCA"></a>1-3、利用Kernel Trick在高维空间做PCA</h3><p>在1-1节，通过PCA，我们得到了$U$矩阵。这里介绍如何仅利用内积的概念来计算传统的PCA。<br>首先需要证明一个结论：$U$可以由$x_1,x_2,\dots,x_N$展开。<br>由于$Cu_a=\lambda_au_a$得<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>u_a &amp; = &amp; \frac&#123;1&#125;&#123;\lambda_a&#125;Cu_a \\\<br>    &amp; = &amp; \frac&#123;1&#125;&#123;\lambda_aN&#125;(\sum_ix_ix_i^T)u_a \\\<br>    &amp; = &amp; \frac&#123;1&#125;&#123;\lambda_aN&#125;\sum_ix_i(x_i^Tu_a) \\\<br>    &amp; = &amp; \frac&#123;1&#125;&#123;\lambda_aN&#125;\sum_i(x_i^Tu_a)x_i \\\<br>    &amp; = &amp; \sum_i\frac&#123;x_i^Tu_a&#125;&#123;\lambda_aN&#125;x_i \\\<br>    &amp; = &amp; \sum_i\alpha_i^ax_i<br>\end&#123;array&#125;<br>$$<br>其中定义$\alpha_i^a=\frac&#123;x_i^Tu_a&#125;&#123;\lambda_aN&#125;$<br>因为$x_i^Tu_a$是一个标量，所以$\alpha_i^a$也是一个标量，因此$u_a$是可以由$x_i$组成的。<br>进而我们介绍PCA投影可以用内积表示，例如我们把$x_i$向任意一个主成分分量$u_a$进行投影，得到的是$u_a^Tx_i$，也就是$x^Tu$。我猜测写成这种形式是为了能抽出$x_i^Tx_j=&lt; x_i , x_j &gt;$的内积形式。<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>x_i^TCu_a &amp; = &amp; \lambda_ax_i^Tu_a \\\<br>x_i^T\frac&#123;1&#125;&#123;N&#125;\sum_jx_jx_j^T\sum_k\alpha_k^ax_k &amp; = &amp; \lambda_ax_i^T\sum_k\alpha_k^ax_k \\\<br>\sum_k\alpha_k^a\sum_j(x_i^Tx_j)(x_j^Tx_k) &amp; = &amp; N\lambda_a\sum_k\alpha_k^a(x_i^Tx_k)<br>\end&#123;array&#125;<br>$$<br>也就是说，对于$i$：<br>$$<br>\alpha_1^a(x_i^Tx_1x_1^Tx_1+x_i^Tx_2x_2^Tx_1+\dots+x_i^Tx_Nx_N^Tx_1)+ \\\<br>\alpha_2^a(x_i^Tx_1x_1^Tx_2+x_i^Tx_2x_2^Tx_2+\dots+x_i^Tx_Nx_N^Tx_2)+\dots \\\<br>\alpha_N^a(x_i^Tx_1x_1^Tx_N+x_i^Tx_2x_2^Tx_N+\dots+x_i^Tx_Nx_N^Tx_N)+ \\\<br>= N\lambda_a(\alpha_1x_i^Tx_1+\alpha_2x_i^Tx_2+\dots+\alpha_Nx_i^Tx_N)<br>$$<br>将其写成矩阵形式为：<br>$$<br>\begin&#123;pmatrix&#125;<br>x_1^Tx_1 &amp; x_1^Tx_2 &amp; \dots &amp; x_1^Tx_N \\\<br>x_2^Tx_1 &amp; x_2^Tx_2 &amp; \dots &amp; x_2^Tx_N \\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\<br>x_N^Tx_1 &amp; x_N^Tx_2 &amp; \dots &amp; x_N^Tx_N<br>\end&#123;pmatrix&#125;<br>\begin&#123;pmatrix&#125;<br>x_1^Tx_1 &amp; x_1^Tx_2 &amp; \dots &amp; x_1^Tx_N \\\<br>x_2^Tx_1 &amp; x_2^Tx_2 &amp; \dots &amp; x_2^Tx_N \\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\<br>x_N^Tx_1 &amp; x_N^Tx_2 &amp; \dots &amp; x_N^Tx_N<br>\end&#123;pmatrix&#125;<br>\begin&#123;pmatrix&#125;<br>\alpha_1^a \\\<br>\alpha_2^a \\\<br>\vdots \\\<br>\alpha_N<br>\end&#123;pmatrix&#125; = N \lambda_a<br>\begin&#123;pmatrix&#125;<br>x_1^Tx_1 &amp; x_1^Tx_2 &amp; \dots &amp; x_1^Tx_N \\\<br>x_2^Tx_1 &amp; x_2^Tx_2 &amp; \dots &amp; x_2^Tx_N \\\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\<br>x_N^Tx_1 &amp; x_N^Tx_2 &amp; \dots &amp; x_N^Tx_N<br>\end&#123;pmatrix&#125;<br>\begin&#123;pmatrix&#125;<br>\alpha_1^a \\\<br>\alpha_2^a \\\<br>\vdots \\\<br>\alpha_N<br>\end&#123;pmatrix&#125;<br>$$<br>当我们定义$K_&#123;ij&#125;=x_i^Tx_j$时，上式可以写成$K^2\alpha^a=N\lambda_aK\alpha^a$，这里定义$\alpha^a=\begin&#123;bmatrix&#125; \alpha_1^a &amp; \alpha_2^a &amp; \dots &amp; \alpha_N^a \end&#123;bmatrix&#125;^T$。<br>进一步，我们得到：<br>$K\alpha^a=\tilde\lambda_a\alpha_a$，其中$\tilde\lambda_a=N\lambda_a$<br>$K$矩阵包含特征值$\tilde\lambda_a$和特征向量$\alpha^a$，我们可以通过$\alpha^a$计算得到特征向量$u_a$。<br>注意特征值分解时，$\alpha_a$只代表一个方向，它的模一般为1，但是在此处不为1。下面计算$\alpha^a$的长度：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>1 &amp; = &amp; u_a^Tu_a \\\<br>  &amp; = &amp; (\sum_i\alpha_i^ax_i)^T(\sum_i\alpha_i^ax_i) \\\<br>  &amp; = &amp; \sum_i\sum_j\alpha_i^a\alpha_j^ax_i^Tx_i \\\<br>  &amp; = &amp; (\alpha^a)^TK\alpha^a \\\<br>  &amp; = &amp; (\alpha^a)^T\tilde\lambda_a\alpha^a \\\<br>  &amp; = &amp; \tilde\lambda_a(\alpha^a)^T\alpha^a<br>\end&#123;array&#125;<br>$$<br>所以<br>$||\alpha^a||=\dfrac&#123;1&#125;&#123;\sqrt&#123;\tilde\lambda_a&#125;&#125;$<br>在上面的分析过程中，我们使用了内积的思想完成了传统PCA的过程，即<strong>可以通过核矩阵$K$的特征值$\tilde\lambda_a$和特征向量$\alpha^a$计算协方差矩阵的特征值$\lambda_a=\frac&#123;\tilde\lambda_a&#125;&#123;N&#125;$和特征向量$u_a=\displaystyle\sum_i^N\frac&#123;\alpha_i^a&#125;&#123;\sqrt&#123;\tilde\lambda_a&#125;&#125;x_i$。</strong></p>
<h3 id="1-4、KPCA的实现"><a href="#1-4、KPCA的实现" class="headerlink" title="1-4、KPCA的实现"></a>1-4、KPCA的实现</h3><p>我们重复1-3过程，看看KPCA中的有关推导。<br>设$\bar C$的特征值和对应的特征向量分别为$\lambda_k$和$u_k$，即<br>$$\bar Cu_k=\lambda_k u_k(k=1,2,\dots,D) \dots \dots \dots \dots ( *) $$<br>由1-3知，$u_k\in span\&#123;\tilde\Phi(x_1),\tilde\Phi(x_2),\dots,\tilde\Phi(x_N)\&#125;$，即存在$\alpha_1^k,\alpha_2^k,\dots,\alpha_N^k$使得<br>$$u_k=\sum_i^N\alpha_i^k\tilde\Phi(x_i)$$<br>用$\tilde\Phi(x_j)^T$同时作用于(*)式子得到：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>\lambda_k\tilde\Phi(x_j)^Tu_k &amp; = &amp; \tilde\Phi(x_j)^T\bar Cu_k<br>\end&#123;array&#125;<br>$$<br>左边可以化为：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>\lambda_k\tilde\Phi(x_j)^Tu_k &amp; = &amp; \lambda_k\left(\sum_i^N\alpha_i^k\tilde\Phi(x_j)^T\tilde\Phi(x_i)\right) \\\<br>                              &amp; = &amp; \lambda_k\left(\sum_i^N\alpha_i^k\tilde K_&#123;ij&#125;\right) \\\<br>                              &amp; = &amp; \lambda_k\left(\tilde K\alpha^k\right)_j\text&#123;     下标j表示矩阵的第j行&#125;<br>\end&#123;array&#125;<br>$$<br>其中<br>$\tilde K=\left(\tilde K_&#123;ij&#125;\right)_&#123;N\times N&#125;=\left(\tilde\Phi(x_i)^T\tilde\Phi(x_i)\right)_&#123;N\times N&#125;$，$\alpha^k=(\alpha_1^k,\alpha_2^k,\dots,\alpha_N^k)^T$<br>右边可以化为：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>\tilde\Phi(x_j)^T\bar Cu_k &amp; = &amp; \tilde\Phi(x_j)^T\frac&#123;1&#125;&#123;N&#125;\sum_l^N\tilde\Phi(x_l)\tilde\Phi(x_l)^T\sum_i^N\alpha_i^k\tilde\Phi(x_i) \\\<br>                           &amp; = &amp; \frac&#123;1&#125;&#123;N&#125;\sum_i^N\sum_l^N\alpha_i^k\left(\tilde\Phi(x_j)^T\tilde\Phi(x_l)\tilde\Phi(x_l)^T\tilde\Phi(x_i)\right) \\\<br>                           &amp; = &amp; \frac&#123;1&#125;&#123;N&#125;\sum_i^N\sum_l^N\alpha_i^k\tilde K_&#123;jl&#125;\tilde K_&#123;li&#125; \\\<br>                           &amp; = &amp; \frac&#123;1&#125;&#123;N&#125;\left(\tilde K^2\alpha^k\right)_j\text&#123;     下标j表示矩阵的第j行&#125;<br>\end&#123;array&#125;<br>$$<br>让$j$依次取遍$1,2,\dots,N$的所有数，得到上面推导的”左边=右边”的矩阵表示：<br>$$\lambda_k\tilde K\alpha^k=\frac&#123;1&#125;&#123;N&#125;\tilde K^2\alpha^k$$<br>化简得到<br>$$\tilde K\alpha^k=N\lambda_k\alpha^k=\tilde\lambda_k\alpha^k$$<br>即之前假设的$\alpha^k=(\alpha_1^k,\alpha_2^k,\dots,\alpha_N^k)^T$为中心化的核矩阵$\tilde K$的第$k$个特征向量和对应的特征值$\tilde\lambda_k$。<strong>现在我们可以用所求的$\tilde\lambda_k$和$\alpha^k$去表示协方差矩阵$\bar C$的特征值$\lambda_k$和特征向量$u_k$。</strong><br>$\lambda_k=\frac&#123;\tilde\lambda_k&#125;&#123;N&#125;$，$u_k=\displaystyle\sum_i^N\alpha_i^k\tilde\Phi(x_i)=\tilde\Phi\alpha^k$，其中$\tilde\Phi=\left(\tilde\Phi(x_1),\tilde\Phi(x_2),\dots,\tilde\Phi(x_N)\right)$<br>现在还有个小问题没有解决，在传统PCA方法中，协方差矩阵得到的特征向量是单位正交的。同样，我们希望$u_k$也是单位向量，即：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>1 &amp; = &amp; u_k^Tu_k \\\<br>  &amp; = &amp; \left(\sum_i^N\alpha_i^k\tilde\Phi(x_i)\right)^T\left(\sum_j^N\alpha_j^k\tilde\Phi(x_j)\right) \\\<br>  &amp; = &amp; (\alpha^k)^T\tilde\Phi^T\tilde\Phi\alpha_k \\\<br>  &amp; = &amp; (\alpha^k)^T\tilde K\alpha_k \\\<br>  &amp; = &amp; (\alpha^k)^T\tilde\lambda_k\alpha_k \\\<br>  &amp; = &amp; \tilde\lambda_k(\alpha^k)^T\alpha_k \\\<br>  &amp; = &amp; \tilde\lambda_k||\alpha^k||^2<br>\end&#123;array&#125;<br>$$<br>因此，将$\tilde K$的特征向量$\alpha^k$的长度归一化到$||\alpha^k||=\dfrac&#123;1&#125;&#123;\sqrt&#123;\tilde\lambda_k&#125;&#125;$，便能保证$u_k$的长度为1。<br>下面证明正交性：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>u_i^Tu_j &amp; = &amp; (\alpha^i)^T\tilde\Phi^T\tilde\Phi\alpha^j \\\<br>         &amp; = &amp; (\alpha^i)^T\tilde K\alpha^j \\\<br>         &amp; = &amp; (\alpha^i)^T\tilde\lambda_j\alpha^j \\\<br>         &amp; = &amp; \tilde\lambda_j(\alpha_i)^T\alpha_j \\\<br>         &amp; = &amp; 0<br>\end&#123;array&#125;<br>$$<br>得证！<br><strong>最后总结一下之前的结论，特征空间中协方差矩阵$\bar C$的特征值为$\lambda_k=\dfrac&#123;\tilde\lambda_k&#125;&#123;N&#125;$，对应的特征向量为$u_k=\dfrac&#123;1&#125;&#123;\sqrt&#123;\tilde\lambda_k&#125;&#125;\tilde\Phi\alpha^k,(k=1,2,\dots,D)$，其中$\tilde\lambda_k$和$\alpha^k$为$\tilde K$的特征值和特征向量。</strong><br>由于映射$\Phi$未知，直接在特征空间内进行数据中心化不可行，直接将输入数据在特征空间中心的过程见1-6。</p>
<h3 id="1-5、在主成分方向上投影"><a href="#1-5、在主成分方向上投影" class="headerlink" title="1-5、在主成分方向上投影"></a>1-5、在主成分方向上投影</h3><p>传统PCA投影时，只需要使用$U$矩阵，假设我们得到的新数据为$T=(t_1,t_2,\dots,t_M)$，那么$t_k$在$u_a$方向的投影是：<br>$$u_a^Tt_k=\sum_i\alpha_i^ax_i^Tt_k=\sum_i\alpha_i^a(x_i^Tt_k)$$<br>对于高维空间的数据$\tilde\Phi(x_i),\tilde\phi(x_j)$，我们可以用Kernel Trick，用$\tilde\Psi(x_i,t_k)=\tilde\Phi(x_i)^T\tilde\phi(t_k)$来代入上式：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>u_k^T\tilde\Phi(t_k) &amp; = &amp; \frac&#123;1&#125;&#123;\sqrt&#123;\tilde\lambda_k&#125;&#125;\left(\sum_i^N\alpha_i^k\tilde\Phi(x_i)\right)^T\tilde\phi(t_k) \\\<br>                     &amp; = &amp; \frac&#123;1&#125;&#123;\sqrt&#123;\tilde\lambda_k&#125;&#125;\sum_i^N\alpha_i^k\left(\tilde\Phi(x_i)^T\tilde\phi(t_k)\right) \\\<br>                     &amp; = &amp; \sum_i^N\frac&#123;\alpha_i^k&#125;&#123;\sqrt&#123;\tilde\lambda_k&#125;&#125;\tilde\Psi(x_i,t_k)<br>\end&#123;array&#125;<br>$$<br>一般$\tilde\phi(t_k)$不能显式得到，从而需要计算输入数据$T$的中心化核矩阵，记为$\tilde\Psi$。</p>
<h3 id="1-6、中心化高维空间的数据"><a href="#1-6、中心化高维空间的数据" class="headerlink" title="1-6、中心化高维空间的数据"></a>1-6、中心化高维空间的数据</h3><p>在我们的分析中，协方差矩阵的定义需要中心化数据。在高维空间中，显式地将$\Phi(x_i)$中心化并不简单，因为我们并不知道$\Phi$的显式表达。但从上面两节可以看出，所有的计算只和$K$矩阵有关。具体计算如下：<br>令$\Phi_i=\Phi(x_i)$，其中：$\tilde\Phi_i=\Phi_i-\frac&#123;1&#125;&#123;N&#125;\displaystyle\sum_k\Phi_k$<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>\tilde K_&#123;ij&#125; &amp; = &amp; &lt;\tilde\Phi_i,\tilde\Phi_j&gt; \\\<br>              &amp; = &amp; (\Phi_i-\frac&#123;1&#125;&#123;N&#125;\sum_k\Phi_k)^T(\Phi_j-\frac&#123;1&#125;&#123;N&#125;\sum_l\Phi_l) \\\<br>              &amp; = &amp; \Phi_i^T\Phi_j-\frac&#123;1&#125;&#123;N&#125;\sum_k\Phi_k^T\Phi_j-\frac&#123;1&#125;&#123;N&#125;\sum_l\Phi_i^T\Phi_l+\frac&#123;1&#125;&#123;N^2&#125;\sum_k\sum_l\Phi_k^T\Phi_l \\\<br>              &amp; = &amp; K_&#123;ij&#125;-\frac&#123;1&#125;&#123;N&#125;\sum_kK_&#123;kj&#125;-\frac&#123;1&#125;&#123;N&#125;\sum_lK_&#123;il&#125;+\frac&#123;1&#125;&#123;N^2&#125;\sum_k\sum_lK_&#123;kl&#125;<br>\end&#123;array&#125;<br>$$<br>不难看出<br>$$\tilde K=K-1_NK-K1_N+1_NK1_N$$<br>其中$1_N$为$N\times N$的矩阵，其中每个元素都是$\dfrac&#123;1&#125;&#123;N&#125;$。<br>对于新的数据，如果数据$t_j\not\in X$：<br>$$<br>\begin&#123;array&#125; &#123;l l l&#125;<br>\tilde\Psi_&#123;i,j&#125; &amp; = &amp; &lt;\tilde\Phi_i,\tilde\phi_j&gt; \\\<br>                 &amp; = &amp; \left(\Phi_i-\frac&#123;1&#125;&#123;N&#125;\sum_k\Phi_k\right)^T\left(\phi_j-\frac&#123;1&#125;&#123;M&#125;\sum_l\phi_l\right) \\\<br>                 &amp; = &amp; \Phi_i^T\phi_j-\frac&#123;1&#125;&#123;N&#125;\sum_k\Phi_k^T\phi_j-\frac&#123;1&#125;&#123;M&#125;\sum_l\Phi_i^T\phi_l+\frac&#123;1&#125;&#123;MN&#125;\sum_k\sum_l\Phi_k^T\phi_l \\\<br>                 &amp; = &amp; \Psi_&#123;i,j&#125;-\frac&#123;1&#125;&#123;N&#125;\sum_k\Psi_&#123;k,j&#125;-\frac&#123;1&#125;&#123;M&#125;\sum_l\Psi_&#123;i,l&#125;+\frac&#123;1&#125;&#123;MN&#125;\sum_k\sum_l\Psi_&#123;k,l&#125;<br>\end&#123;array&#125;<br>$$<br>不难看出<br>$$\tilde\Psi=\Psi-1_NK-K1_M+1_NK1_M$$<br>如果有数据$t_k\in X$，则上式中有$M=N$，且$\tilde K=\tilde\Psi$。</p>
<h2 id="2、算法实现"><a href="#2、算法实现" class="headerlink" title="2、算法实现"></a>2、算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kpca</span><span class="params">(dataMat, kTup, topN = <span class="number">99</span>)</span>:</span></div><div class="line">	M, N = shape(dataMat)</div><div class="line">	K = mat(zeros((M, M)))</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(M):</div><div class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> range(M):</div><div class="line">			K[i, j] = kernel(dataMat[i, :], dataMat[j, :], kTup)</div><div class="line">	M_M = mat(ones((M, M))) / M</div><div class="line">	cenK = K - M_M * K - K * M_M + M_M * K * M_M</div><div class="line">	eigVals, eigVects = linalg.eig(cenK)</div><div class="line">	eigValInd = argsort(eigVals)</div><div class="line">	eigValInd = eigValInd[<span class="number">-1</span>: -(topN + <span class="number">1</span>): <span class="number">-1</span>]</div><div class="line">	redEigVects = eigVects[:, eigValInd]</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(topN):</div><div class="line">		redEigVects[:, i] = redEigVects[:, i] / sqrt(eigVals[eigValInd[i]])</div><div class="line">	needMat = cenK * redEigVects</div><div class="line">	<span class="keyword">return</span> needMat</div></pre></td></tr></table></figure>
<h2 id="3、说明"><a href="#3、说明" class="headerlink" title="3、说明"></a>3、说明</h2><p>PCA是一种无参数技术，但是KPCA显然不是。</p>

  </section>

  

<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'shengtao96'; 
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d81ff4fc458aba1722010bdb220f0103";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>

<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      机器学习笔记（第一周） | 鲭兜的博客 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="shengtao96">
    
    

    <meta name="description" content="1、机器学习概述1-1、机器学习定义Two definitions of Machine Learning are offered.Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.A computer prog">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（第一周） | 鲭兜的博客">
<meta property="og:url" content="http://shengtao96.github.io/2016/12/11/机器学习笔记（第一周）/index.html">
<meta property="og:site_name" content="鲭兜的博客">
<meta property="og:description" content="1、机器学习概述1-1、机器学习定义Two definitions of Machine Learning are offered.Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.A computer prog">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/9.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/20.png">
<meta property="og:updated_time" content="2017-05-29T06:13:09.897Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记（第一周） | 鲭兜的博客">
<meta name="twitter:description" content="1、机器学习概述1-1、机器学习定义Two definitions of Machine Learning are offered.Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.A computer prog">
<meta name="twitter:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">鲭兜的博客</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          努力に胜る天才无し
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/shengtao96" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">机器学习笔记（第一周）</h1>

    

    <div class="post-meta">
      <time datetime="2016-12-11" class="post-meta__date date">2016-12-11</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/程序猿之路净化一切/">程序猿之路净化一切</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/数学/">数学</a>, <a class="tags-link" href="/tags/机器学习/">机器学习</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=479979316&auto=1&height=66"></iframe>

<h2 id="1、机器学习概述"><a href="#1、机器学习概述" class="headerlink" title="1、机器学习概述"></a>1、机器学习概述</h2><h3 id="1-1、机器学习定义"><a href="#1-1、机器学习定义" class="headerlink" title="1-1、机器学习定义"></a>1-1、机器学习定义</h3><p>Two definitions of Machine Learning are offered.<br>Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.<br>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<h3 id="1-2、机器学习分类"><a href="#1-2、机器学习分类" class="headerlink" title="1-2、机器学习分类"></a>1-2、机器学习分类</h3><p>机器学习问题可以大致被分为监督学习（supervised learning）和非监督学习（unsupervised learning），其它还有强化学习（reinforcement learning）和推荐系统（recommender systems）。</p>
<h3 id="1-3、Supervised-Learning-监督学习"><a href="#1-3、Supervised-Learning-监督学习" class="headerlink" title="1-3、Supervised Learning 监督学习"></a>1-3、Supervised Learning 监督学习</h3><p>We give the algorithm a dataset in which the ‘right answers’ are given, the algorithm is to produce more of these right answers.<br>给一个算法，需要部分数据集已经有正确答案，根据算法分析已知数据集从而得到更多数据的“正确答案”。<br>监督学习又可以被分为回归问题和分类问题两类:</p>
<h4 id="1-3-1、Regression-Problem-回归问题"><a href="#1-3-1、Regression-Problem-回归问题" class="headerlink" title="1-3-1、Regression Problem 回归问题"></a>1-3-1、Regression Problem 回归问题</h4><p>predict continuous valued output 预测一个连续值输出<br>典型例子:房价预测 Housing Price Prediction</p>
<h4 id="1-3-2、Classification-Problem-分类问题"><a href="#1-3-2、Classification-Problem-分类问题" class="headerlink" title="1-3-2、Classification Problem 分类问题"></a>1-3-2、Classification Problem 分类问题</h4><p>predict discrete valued output 预测一个离散值输出<br>典型例子:判断良性恶性肿瘤<br>在分类问题中，有时会有超过两个的输出值（比如无癌症、胃癌、肺癌、肝癌），但仍然是可数的，离散的。<br>在分类问题中，可能有不止一个的特征和属性，但是算法能够处理无穷多个特征，利用一种叫Support Vector Machine 支持向量机的算法（一个简洁的数学方法，能够让电脑处理无限多的特征）。</p>
<h3 id="1-4、Unsupervised-Learning-非监督学习"><a href="#1-4、Unsupervised-Learning-非监督学习" class="headerlink" title="1-4、Unsupervised Learning 非监督学习"></a>1-4、Unsupervised Learning 非监督学习</h3><p>We can derive this structure by clustering the data based on relationships among the variables in the data.<br>非监督学习是一种学习机制，你给算法大量的数据，要求它找出数据中蕴含的类型结构。<br>在非监督学习中，数据没有属性或者标签的概念，所有的数据都是一样的，没有任何区别（而你的目标就是找出区别，并且把数据分类）。<br>With unsupervised learning there is no feedback based on the prediction results.<br>非监督学习主要可以分为聚类算法问题和非聚类算法问题两种：</p>
<h4 id="1-4-1、Clustering-Algorithm-聚类算法"><a href="#1-4-1、Clustering-Algorithm-聚类算法" class="headerlink" title="1-4-1、Clustering Algorithm 聚类算法"></a>1-4-1、Clustering Algorithm 聚类算法</h4><p>将一个数据集分成几个聚类，这就是聚类算法。<br>典型例子是google新闻，它把成千上万的新闻聚集起来，按照每个新闻的内容分成一个一个的专题。</p>
<h4 id="1-4-2、Cocktail-Party-Algorithm-鸡尾酒会算法（Non-clustering-Algorithm-非聚类算法）"><a href="#1-4-2、Cocktail-Party-Algorithm-鸡尾酒会算法（Non-clustering-Algorithm-非聚类算法）" class="headerlink" title="1-4-2、Cocktail Party Algorithm 鸡尾酒会算法（Non-clustering Algorithm 非聚类算法）"></a>1-4-2、Cocktail Party Algorithm 鸡尾酒会算法（Non-clustering Algorithm 非聚类算法）</h4><p><strong>鸡尾酒宴问题</strong><br>有一个宴会，有一屋子的人，大家都坐在一起，而且在同时说话，有许多声音混杂在一起，因为每个人都是在同一时间说话的，在这种情况下你很难听清楚你面前的人说的话。因此，比如有这样一个场景，宴会上只有两个人，同时说话， 有两个麦克风，把它们放在房间里，然后因为这两个麦克风距离这两个人的距离不同，每个麦克风都记录下了来自两个人的声音的不同组合。A的声音在第一个麦克风里的声音会响一点，B的声音在第二个麦克风里会比较响一点，因为两个麦克风的位置相对于两个说话者的位置是不同的。但每个麦克风都会录到来自两个说话者的重叠部分的声音。<br>Cocktail Party Algorithm<br>Separate out these two audio sources that were being added or being summed together to from other recordings.<br>鸡尾酒会算法：找出其中蕴含的分类，算法还会分离出两个被叠加到一起的音频源。<br>代码：$[W,s,v]=svd((repmat(sum(x.<em>x,1),size(x,1),1).</em>x)*x’);$</p>
<h2 id="2、Linear-Regression-with-One-Variable-单变量线性回归"><a href="#2、Linear-Regression-with-One-Variable-单变量线性回归" class="headerlink" title="2、Linear Regression with One Variable 单变量线性回归"></a>2、Linear Regression with One Variable 单变量线性回归</h2><h3 id="Notation-部分使用符号"><a href="#Notation-部分使用符号" class="headerlink" title="Notation 部分使用符号"></a>Notation 部分使用符号</h3><p>$m$ = Number of training examples 训练样本的数目<br>$x’s$ = “input” variable / features  输入变量/特征量<br>$y’s$ = “output” variable / “target” variable 输出变量/目标变量<br>$(x,y)$ = one training example 某一个训练样本（泛指）<br>$(x^&#123;(i)&#125;,y^&#123;(i)&#125;)$ ith training example 第i个训练样本（特指）<br>Training Set: a list of $m$ training examples $(x^&#123;(i)&#125; , y^&#123;(i)&#125; ); i =1, . . . , m$</p>
<h3 id="2-1、Model-Representation-模型表示"><a href="#2-1、Model-Representation-模型表示" class="headerlink" title="2-1、Model Representation 模型表示"></a>2-1、Model Representation 模型表示</h3><p>我们将训练集数据交给学习算法，将会得到一个函数，通常用h来表示,h则代表hypothesis 假设。对于函数h，输入变量x’s,得到输出y’s。<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png" alt="无法加载图片"></p>
<h3 id="2-2、Linear-Regression-with-One-Variable-Model-Univariate-Linear-Regression-单变量线性回归模型"><a href="#2-2、Linear-Regression-with-One-Variable-Model-Univariate-Linear-Regression-单变量线性回归模型" class="headerlink" title="2-2、Linear Regression with One Variable Model (Univariate Linear Regression)单变量线性回归模型"></a>2-2、Linear Regression with One Variable Model (Univariate Linear Regression)单变量线性回归模型</h3><p>我们以这个公式来表示函数h：$h_\theta(x)=\theta_0+\theta_1x$<br>我们要预测一个关于x的线性函数，当然非线性函数也是可能的。</p>
<h3 id="2-3、Cost-Function-代价函数"><a href="#2-3、Cost-Function-代价函数" class="headerlink" title="2-3、Cost Function 代价函数"></a>2-3、Cost Function 代价函数</h3><p>$\theta_i’s$ = the parameters of the model 模型参数<br>我们要选择能使$h(x)$，也就是输入$x$时我们预测的值最接近该样本对应的y值的参数$\theta_0,\theta_1$。<br>在线性回归中，我们要解决的是一个最小化问题。<br>$$\displaystyle\min_&#123;\theta_0,\theta_1&#125;\dfrac&#123;1&#125;&#123;2m&#125;\sum_&#123;i=1&#125;^m(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2$$<br>这是线性回归的整体目标函数，但是为了方便，我们定义一个代价函数。<br>$$\begin&#123;cases&#125;<br>        J(\theta_0,\theta_1)=\dfrac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2\\<br>        \min_&#123;\theta_0,\theta_1&#125;J(\theta_0,\theta_1)<br>\end&#123;cases&#125;$$<br>我们要做的就是关于$\theta_0$和$\theta_1$，对函数$J(\theta_0,\theta_1)$求最小值，这个函数也被称为Squared Error Function 平方误差函数。<br>平方误差代价函数对于大多数问题，特别是回归问题，都是一个合理的选择，还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。</p>
<h4 id="2-3-1、直观理解一"><a href="#2-3-1、直观理解一" class="headerlink" title="2-3-1、直观理解一"></a>2-3-1、直观理解一</h4><p>为了更好理解假设函数与代价函数，不妨假设$\theta_0=0$，即$h_\theta(x)=\theta_1x$，此时假设函数只有一个模型参数$\theta_1$。<br>对应的代价函数也变成了$J(\theta_1)=\dfrac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2=\dfrac&#123;1&#125;&#123;2m&#125;\sum_&#123;i=1&#125;^m(\theta_1x^&#123;(i)&#125;-y^&#123;(i)&#125;)^2$，也就是说现在代价函数只有一个自变量，从图形上来理解就是我们选择的假设函数会经过（0，0）点。</p>
<h4 id="2-3-2、直观理解二"><a href="#2-3-2、直观理解二" class="headerlink" title="2-3-2、直观理解二"></a>2-3-2、直观理解二</h4><p>一般的，我们考察$J(\theta_0,\theta_1)$函数的图形，我们会发现函数图形（三维）是呈现一个倒置伞状的（实际上这取决于训练样本）。<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/3.png" alt="无法加载图片"><br>还有一种图形表现是contour plot 轮廓图<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/4.png" alt="无法加载图片"><br>两个轴分别表示两个模型参数，图表里面包含一圈一圈的椭圆形，每一个圈就表示函数值相同的所有点的集合。理解的话，想象刚才的伞形图形从屏幕中冒出来，在同一时刻伞形图形与屏幕相交的边就是这些一圈一圈的椭圆形，在椭圆上的每个点函数值相同。（可以类比高中地理的等势图）<br>对于多维度、多参数的代价函数，我们很难实现它的图形化，但我们可以编写程序来自动地找到使代价函数最小的参数组合。</p>
<h3 id="2-4、Gradient-Descent-梯度下降算法"><a href="#2-4、Gradient-Descent-梯度下降算法" class="headerlink" title="2-4、Gradient Descent 梯度下降算法"></a>2-4、Gradient Descent 梯度下降算法</h3><p>梯度下降算法可以将代价函数$J$最小化，它是很常用的算法，它不仅被用在线性回归上。<br>假设有某个代价函数$J(\theta_0,\theta_1)$，想要最小化这个函数。这个函数可以是一个线性回归的代价函数，也可以是一些其他函数。当然，梯度下降算法不止使用于两个参数的情况，可以使用于多个参数的情况，梯度下降算法可以解决更加一般的问题。<br>但是，需要注意的是，使用梯度下降算法也必须满足一定的条件。需要被最小化的函数一定是一个凸函数（也就是呈一个伞状，即没有区部最优值），如果不满足这个条件的话，只能使用模拟退火算法。</p>
<h4 id="2-4-1、Outline-构想"><a href="#2-4-1、Outline-构想" class="headerlink" title="2-4-1、Outline 构想"></a>2-4-1、Outline 构想</h4><p>1、Start with some $\theta_0,\theta_1$：对$\theta_0$和$\theta_1$进行初步的猜测（通常选择将$\theta_0$和$\theta_1$都设为0）。<br>2、Keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at a minimum 不停地改变$\theta_0$和$\theta_1$，试图通过这种改变使得$J(\theta_0,\theta_1)$变小，直到我们找到J的最小值。<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/9.png" alt="无法加载图片"></p>
<h4 id="2-4-2、特点"><a href="#2-4-2、特点" class="headerlink" title="2-4-2、特点"></a>2-4-2、特点</h4><p>起始点的位置略有不同，你就可能会得到一个不同的局部最优解，这是梯度下降算法的一大特点。</p>
<h4 id="2-4-3、算法定义"><a href="#2-4-3、算法定义" class="headerlink" title="2-4-3、算法定义"></a>2-4-3、算法定义</h4><p>$\text&#123;repeat until convergence [&#125;\\<br>　　　　\theta_j:=&#123;\theta&#125;_j-\alpha\dfrac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_j&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)　　　(for:j=0　\text&#123;and&#125;　:j=1)\\<br>\text&#123;]&#125;$</p>
<h4 id="2-4-4、说明"><a href="#2-4-4、说明" class="headerlink" title="2-4-4、说明"></a>2-4-4、说明</h4><p>$:=$表示赋值运算符。<br>$=$ 表示相等断言，即声明$a$的值与$b$的值相同。<br>$\alpha$ 是一个常数，被称为learning rate 学习速率。在梯度下降算法中，它控制了我们下降时会迈出的步长。<br>在这个等式中，我们需要同时更新$\theta_0$和$\theta_1$，而不是两者单独更新。<br>Correct: Simlultaneous update<br>$temp0:=\theta_0-\alpha\dfrac&#123;\partial&#125;&#123;\partial\theta_0&#125;J(\theta_0,\theta_1)$<br>$temp1:=\theta_1-\alpha\dfrac&#123;\partial&#125;&#123;\partial\theta_1&#125;J(\theta_0,\theta_1)$<br>$\theta_0:=temp0$<br>$\theta_1:=temp1$</p>
<h4 id="2-4-5、直观理解"><a href="#2-4-5、直观理解" class="headerlink" title="2-4-5、直观理解"></a>2-4-5、直观理解</h4><p>根据导数的图形意义和公式的数学定义，可以看出$\theta$点是向局部最小点趋近的。<br>$\alpha$太小，收敛速度会很慢。<br>$\alpha$太大，那么梯度下降算法可能会越过最低点，甚至因无法收敛而导致发散。<br>当$\theta$点已经收敛于局部最小值，而局部最优点的导数将等于0，所以在梯度下降的过程中，模型参数将不会改变。<br>梯度下降算法能收敛到局部最小值，即便学习率$\alpha$是固定的。当$\theta$点接近局部最小值的时候，梯度下降算法会自动地采取更小的步伐（下山）。所以，我们不需要手动减小$\alpha$。</p>
<h3 id="2-4、线性回归"><a href="#2-4、线性回归" class="headerlink" title="2-4、线性回归"></a>2-4、线性回归</h3><p>将梯度下降和代价函数结合起来，得到具体的拟合直线的线性回归算法，并用梯度下降的方法来最小化平方误差代价函数。<br>那么，我们首先就需要弄清楚梯度下降算法中的偏导项：<br>$$\dfrac&#123;\partial&#125;&#123;\partial\theta_j&#125;J(\theta_0,\theta_1)=\dfrac&#123;\partial&#125;&#123;\partial\theta_j&#125;\dfrac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^&#123;m&#125;(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2\\=\dfrac&#123;\partial&#125;&#123;\partial\theta_j&#125;\dfrac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(\theta_0+\theta_1x^&#123;(i)&#125;-y^&#123;(i)&#125;)^2$$<br>$$j=0:　　\dfrac&#123;\partial&#125;&#123;\partial\theta_0&#125;J(\theta_0,\theta_1)=\dfrac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)$$<br>$$j=1:　\dfrac&#123;\partial&#125;&#123;\partial\theta_1&#125;J(\theta_0,\theta_1)=\dfrac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_\theta(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)x^&#123;(i)&#125;$$<br>下面写出$\theta_j$式子的偏导过程：<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/20.png" alt="无法加载图片"><br>将得到的导数项代回式子，得到<br>$\text&#123;repeat until convergence [&#125;\\<br>　　　　&#123;\theta&#125;_0:=&#123;\theta&#125;_0-\alpha\dfrac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;) \\<br>　　　　&#123;\theta&#125;_1:=&#123;\theta&#125;_1-\alpha\dfrac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)x^&#123;(i)&#125; \\<br>\text&#123;]&#125; \\<br>\text&#123;update $\theta_0$ and $\theta_1$ simultaneously&#125;$<br>对于线性回归模型，它的图形永远是呈现出倒置伞形的，即它的代价函数是一个凸函数，它只有一个全局最优解，所以适用梯度下降算法。</p>
<h3 id="2-5、“Batch”-Gradient-Descent-批量梯度下降"><a href="#2-5、“Batch”-Gradient-Descent-批量梯度下降" class="headerlink" title="2-5、“Batch” Gradient Descent 批量梯度下降"></a>2-5、“Batch” Gradient Descent 批量梯度下降</h3><p>“Batch”：Each step of gradient descent uses all the training examples. 在梯度下降的每一步中，我们都用到了所有的训练样本。<br>所以这里使用的梯度下降算法又叫批量梯度下降算法。<br>但是缺点也同样很明显，当训练集较为庞大时，运行梯度下降的时间将大大增加。</p>
<h3 id="2-6、Normal-Equations-正规方程"><a href="#2-6、Normal-Equations-正规方程" class="headerlink" title="2-6、Normal Equations 正规方程"></a>2-6、Normal Equations 正规方程</h3><p>还有另一种求解代价函数最小值的数值解法，不需要梯度下降这种迭代算法。它可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值。实际上，在数据量较大的情况下，梯度下降比正规方程要更适用一些。</p>

  </section>

  

<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'shengtao96'; 
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d81ff4fc458aba1722010bdb220f0103";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>

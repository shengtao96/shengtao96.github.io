<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      机器学习笔记（第一周） | 鲭鱼香菜的博客 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="shengtao96">
    
    

    <meta name="description" content="1、相关术语spam filter 垃圾邮件过滤器neural networks 神经网络the state of the art 前沿理论database mining 数据库挖掘web click data(clickstream data) 有关网络点击的数据Natrual Language Processing(NLP) 自然语言处理computer vision 计算机视觉self-cu">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（第一周） | 鲭鱼香菜的博客">
<meta property="og:url" content="http://shengtao96.github.io/2016/12/11/机器学习笔记（第一周）/index.html">
<meta property="og:site_name" content="鲭鱼香菜的博客">
<meta property="og:description" content="1、相关术语spam filter 垃圾邮件过滤器neural networks 神经网络the state of the art 前沿理论database mining 数据库挖掘web click data(clickstream data) 有关网络点击的数据Natrual Language Processing(NLP) 自然语言处理computer vision 计算机视觉self-cu">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/4.png">
<meta property="og:updated_time" content="2016-12-17T13:36:40.168Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记（第一周） | 鲭鱼香菜的博客">
<meta name="twitter:description" content="1、相关术语spam filter 垃圾邮件过滤器neural networks 神经网络the state of the art 前沿理论database mining 数据库挖掘web click data(clickstream data) 有关网络点击的数据Natrual Language Processing(NLP) 自然语言处理computer vision 计算机视觉self-cu">
<meta name="twitter:image" content="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">鲭鱼香菜的博客</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          逃避虽然可耻但是有用
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/shengtao96" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

    <!-- China social icon -->
    <!--
    
      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>

      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    -->



  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">机器学习笔记（第一周）</h1>

    

    <div class="post-meta">
      <time datetime="2016-12-11" class="post-meta__date date">2016-12-11</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/程序猿之路净化一切/">程序猿之路净化一切</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/数学/">数学</a>, <a class="tags-link" href="/tags/机器学习/">机器学习</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h2 id="1、相关术语"><a href="#1、相关术语" class="headerlink" title="1、相关术语"></a>1、相关术语</h2><p>spam filter 垃圾邮件过滤器<br>neural networks 神经网络<br>the state of the art 前沿理论<br>database mining 数据库挖掘<br>web click data(clickstream data) 有关网络点击的数据<br>Natrual Language Processing(NLP) 自然语言处理<br>computer vision 计算机视觉<br>self-customizing programs 自定制化程序<br>organize computing clusters 组织计算机集群<br>social network analysis 社交网络分析<br>market segmentation 市场分割<br>contour plot / contour figure 轮廓图<br>cluster 簇、聚类<br>convergence 收敛<br>singular value decomposition 奇异值分解</p>
<h2 id="2、笔记内容"><a href="#2、笔记内容" class="headerlink" title="2、笔记内容"></a>2、笔记内容</h2><h3 id="2-1、机器学习定义"><a href="#2-1、机器学习定义" class="headerlink" title="2-1、机器学习定义"></a>2-1、机器学习定义</h3><p>Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.<br>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<h3 id="2-2、机器学习分类"><a href="#2-2、机器学习分类" class="headerlink" title="2-2、机器学习分类"></a>2-2、机器学习分类</h3><p>机器学习问题可以被分为监督学习和非监督学习两类</p>
<h3 id="2-3、supervised-learning-监督学习"><a href="#2-3、supervised-learning-监督学习" class="headerlink" title="2-3、supervised learning 监督学习"></a>2-3、supervised learning 监督学习</h3><p>we gave the algorithm a data set in which the ‘right answers’ were given.<br>给出一个算法，需要部分数据集已经有正确答案，根据分析已知数据集得到更多数据的“正确答案”。<br>In supervised learning, in every example in our data set, we are told what is the ‘correct answer’ that we would have quite liked the algorithms have predicted on that example.<br>监督学习中，对于数据集中每一个数据，都有相应的正确答案（训练集），算法基于这些来做出预测。<br>training set 训练集：已知“正确答案”的数据集合</p>
<h4 id="Notation-部分使用符号"><a href="#Notation-部分使用符号" class="headerlink" title="Notation 部分使用符号"></a>Notation 部分使用符号</h4><p>m = Number of training examples 训练样本的数目<br>x’s = “input” variable / features  输入变量/特征量<br>y’s = “output” variable / “target” variable 输出变量/目标变量<br>(x,y) = one training example 一个训练样本<br>$(x^&#123;(i)&#125;,y^&#123;(i)&#125;)$ ith training example 第i个训练样本<br>监督学习可以被分为回归问题和分类问题两类:</p>
<h4 id="2-3-1、Regression-回归问题"><a href="#2-3-1、Regression-回归问题" class="headerlink" title="2-3-1、Regression 回归问题"></a>2-3-1、Regression 回归问题</h4><p>predict continuous valued output 预测一个连续值输出<br>典型例子:房价预测 Housing price prediction</p>
<h4 id="2-3-2、Classification-分类问题"><a href="#2-3-2、Classification-分类问题" class="headerlink" title="2-3-2、Classification 分类问题"></a>2-3-2、Classification 分类问题</h4><p>predict discrete valued output 预测一个离散值输出<br>典型例子:判断良性恶性肿瘤<br>在分类问题中，有时会有超过两个的输出值，但仍然是可数的，离散的<br>在分类问题中，可能有不止一个的特征和属性，而且算法能够处理无穷多个特性，使用一种叫Support Vector Machine 支持向量机的算法（一个简洁的数学方法，能够让电脑处理无限多的特征）。</p>
<h3 id="2-4、unsupervised-learning-非监督学习"><a href="#2-4、unsupervised-learning-非监督学习" class="headerlink" title="2-4、unsupervised learning 非监督学习"></a>2-4、unsupervised learning 非监督学习</h3><p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.<br>We’re given the data set and we’re not told what to do with it and we’re not told what each data point is. Can you find some structure in the data?<br>非监督学习是一种学习机制，你给算法大量的数据，要求它找出数据中蕴含的类型结构<br>don’t have any labels or all has the same label or really no labels<br>在非监督学习中，没有属性或者标签的概念，所有的数据都是一样的，没有区别</p>
<h4 id="2-4-1、聚类算法"><a href="#2-4-1、聚类算法" class="headerlink" title="2-4-1、聚类算法"></a>2-4-1、聚类算法</h4><p>把一个数据集（非监督学习算法只有一个数据集）分成几个聚类，这就是聚类算法。典型例子是google新闻，把成千上万的新闻聚集起来形成一个一个的专题。</p>
<h4 id="2-4-2、Cocktail-party-problem-algorithm-鸡尾酒会算法"><a href="#2-4-2、Cocktail-party-problem-algorithm-鸡尾酒会算法" class="headerlink" title="2-4-2、Cocktail party problem algorithm 鸡尾酒会算法"></a>2-4-2、Cocktail party problem algorithm 鸡尾酒会算法</h4><p>鸡尾酒宴问题<br>有一个宴会，有一屋子的人，大家都坐在一起，而且在同时说话，有许多声音混杂在一起，因为每个人都是在同一时间说话的，在这种情况下你很难听清楚你面前的人说的话。因此，比如有这样一个场景，宴会上只有两个人，同时说话， 有两个麦克风，把它们放在房间里，然后因为这两个麦克风距离这两个人的距离不同，每个麦克风都记录下了来自两个人的声音的不同组合。也许A的声音在第一个麦克风里的声音会响一点，也许B的声音在第二个麦克风里会比较响一点，因为两个麦克风的位置相对于两个说话者的位置是不同的。但每个麦克风都会录到来自两个说话者的重叠部分的声音。<br>Cocktail Party Algorithm<br>separate out these two audio sources that were being added or being summed together to from other recordings<br>鸡尾酒会算法：找出其中蕴含的分类，算法还会分离出两个被叠加到一起的音频源<br>代码：$[W,s,v]=svd((repmat(sum(x.<em>x,1),size(x,1),1).</em>x)*x’);$<br>（svd奇异值分解，解线性方程一个惯例）</p>
<h3 id="2-5、单变量线性回归"><a href="#2-5、单变量线性回归" class="headerlink" title="2-5、单变量线性回归"></a>2-5、单变量线性回归</h3><h4 id="2-5-1、Model-Representation-模型表示"><a href="#2-5-1、Model-Representation-模型表示" class="headerlink" title="2-5-1、Model Representation 模型表示"></a>2-5-1、Model Representation 模型表示</h4><p>我们将训练集数据交给学习算法，将会得到一个函数，通常用h来表示,h则代表hypothesis 假设。对于函数h，输入变量x’s,得到输出y’s<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/learningalgorithm.png" alt="无法加载图片"></p>
<h4 id="2-5-2、linear-regression-with-one-variable-model-univariate-linear-regression-单变量线性回归模型"><a href="#2-5-2、linear-regression-with-one-variable-model-univariate-linear-regression-单变量线性回归模型" class="headerlink" title="2-5-2、linear regression with one variable model (univariate linear regression)单变量线性回归模型"></a>2-5-2、linear regression with one variable model (univariate linear regression)单变量线性回归模型</h4><p>我们以下面这个公式来表示函数h： $h_&#123;\theta&#125;(x)=&#123;\theta&#125;_0+&#123;\theta&#125;_1x$<br>我们要预测一个关于x的线性函数，当然非线性函数也是可能的。</p>
<h4 id="2-5-3、Cost-Function-代价函数"><a href="#2-5-3、Cost-Function-代价函数" class="headerlink" title="2-5-3、Cost Function 代价函数"></a>2-5-3、Cost Function 代价函数</h4><p>$&#123;\theta&#125;_i’s$ 模型参数<br>我们要选择能使$h(x)$也就是输入$x$时我们预测的值最接近该样本对应的y值的参数$&#123;\theta&#125;_0,&#123;\theta&#125;_1$<br>在线性回归中，我们要解决的是一个最小化问题<br>$minimize_&#123;&#123;\theta&#125;_0&#123;\theta&#125;_1&#125;\frac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2$<br>这是线性回归的整体目标函数<br>但是为了方便，我们定义一个代价函数<br>$J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)=\frac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2$<br>我们要做的就是关于$&#123;\theta&#125;_0$和$&#123;\theta&#125;_1$，对函数$J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$求最小值，这个函数也被称为Squared error function 平方误差函数。<br>平方误差代价函数对于大多数问题，特别是回归问题，都是一个合理的选择，还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。</p>
<h5 id="2-5-3-1、直观理解一"><a href="#2-5-3-1、直观理解一" class="headerlink" title="2-5-3-1、直观理解一"></a>2-5-3-1、直观理解一</h5><p>为了更好理解假设函数与代价函数，不妨假设$&#123;\theta&#125;_0=0$<br>即$h_&#123;\theta&#125;(x)=&#123;\theta&#125;_1x$<br>此时假设函数只有一个模型参数$&#123;\theta&#125;_1$<br>对应的代价函数也变成了$J(&#123;\theta&#125;_1)=\frac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2$,也就是说现在代价函数只有一个自变量<br>以图形化来理解就是我们选择的假设函数会经过原点</p>
<h5 id="2-5-3-2、直观理解二"><a href="#2-5-3-2、直观理解二" class="headerlink" title="2-5-3-2、直观理解二"></a>2-5-3-2、直观理解二</h5><p>一般的，我们考察$J(&#123;\theta&#125;_0,&#123;\theta_1&#125;)$函数的图形，我们会发现函数图形（三维）是呈现一个伞状的（实际上这取决于训练样本）。<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/3.png" alt="无法加载图片"><br>还有一种图形表现是contour plot 轮廓图<br><img src="https://raw.githubusercontent.com/shengtao96/picture/master/4.png" alt="无法加载图片"><br>两个轴分别表示两个模型参数，图表里面包含一圈一圈的椭圆形，每一个圈就表示函数值相同的所有点的集合。理解的话，想象刚才的伞形图形从屏幕中冒出来，在同一时刻伞形图形与屏幕相交的边就是这些一圈一圈的椭圆形，在椭圆上的个点函数值相同。<br>（对于多维度、多参数的代价函数，我们很难使之图形化，但我们可以编写程序来自动地找到使代价函数最小的参数组合）</p>
<h4 id="2-5-4、Gradient-Descent-梯度下降"><a href="#2-5-4、Gradient-Descent-梯度下降" class="headerlink" title="2-5-4、Gradient Descent 梯度下降"></a>2-5-4、Gradient Descent 梯度下降</h4><p>梯度下降算法可以将代价函数J最小化，它是很常用的算法，它不仅被用在线性回归上。<br>假设有某个代价函数$J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$,想要最小化这个函数。也许这是一个线性回归的代价函数，也许是一些其他函数。当然，梯度下降算法不只使用于两个参数的情况，可以使用于多个参数的情况，梯度下降算法可以解决更加一般的问题。</p>
<h5 id="2-5-4-1、Outline-构想"><a href="#2-5-4-1、Outline-构想" class="headerlink" title="2-5-4-1、Outline 构想"></a>2-5-4-1、Outline 构想</h5><p>1、Start with some $&#123;\theta&#125;_0,&#123;\theta&#125;_1$ 对$&#123;\theta&#125;_0$和$&#123;\theta&#125;_1$进行初步的猜测（通常选择将$&#123;\theta&#125;_0$设为0，将$&#123;theta&#125;_1$也设为0）<br>2、Keep changing $&#123;theta&#125;_0,&#123;\theta&#125;_1$ to reduce $J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$ until we hopefully end up at a minimum 不停地一点点改变$&#123;\theta&#125;_0$和$&#123;\theta&#125;_1$试图通过这种改变使得$J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$变小，直到我们找到J的最小值或许是局部最小值。</p>
<h5 id="2-5-4-2、特点"><a href="#2-5-4-2、特点" class="headerlink" title="2-5-4-2、特点"></a>2-5-4-2、特点</h5><p>起始点的位置略有不同，你会得到一个非常不同的局部最优解，这是梯度下降算法的一个特点。</p>
<h5 id="2-5-4-3、算法定义"><a href="#2-5-4-3、算法定义" class="headerlink" title="2-5-4-3、算法定义"></a>2-5-4-3、算法定义</h5><p>repeat until convergence&#123;<br>&emsp;$&#123;\theta&#125;_j:=&#123;\theta&#125;_j-\alpha\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_j&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1),(for:j=0and:j=1)$<br>&#125;</p>
<h5 id="2-5-4-4、说明"><a href="#2-5-4-4、说明" class="headerlink" title="2-5-4-4、说明"></a>2-5-4-4、说明</h5><p>:=表示赋值运算符<br>=表示相等断言，即声明a的值与b的值相同<br>$\alpha$是一个常数，被称为learning rate 学习速率。在梯度下降算法中，它控制了我们下降时会迈出的步长。<br>在这个等式中，我们需要同时更新$&#123;\theta&#125;_0$和$&#123;\theta&#125;_1$，而不是两者单独更新<br>Correct: Simlultaneous update<br>$temp0:=&#123;\theta&#125;_0-\alpha\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_0&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$<br>$temp1:=&#123;\theta&#125;_1-\alpha\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_1&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)$<br>$&#123;\theta&#125;_0:=temp0$<br>$&#123;\theta&#125;_1:=temp1$</p>
<h6 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h6><p>根据导数的意义和公式的数学定义，很简单就可以看出$\theta$点是向局部最小点趋近的。<br>$\alpha$太小，收敛速度会很慢<br>$\alpha$太大，那么梯度下降算法可能会越过最低点，甚至无法收敛而导致发散</p>
<h6 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h6><p>非常非常像爬山算法，算法本身有缺陷，改进算法是模拟退火算法。</p>
<h4 id="2-5-4、线性回归"><a href="#2-5-4、线性回归" class="headerlink" title="2-5-4、线性回归"></a>2-5-4、线性回归</h4><p>将梯度下降和代价函数结合起来，得到具体的拟合直线的线性回归算法<br>用梯度下降的方法来最小化平方误差代价函数<br>$\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_j&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)=\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_j&#125;\frac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^&#123;m&#125;(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)^2\\=\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_j&#125;\frac&#123;1&#125;&#123;2m&#125;\displaystyle\sum_&#123;i=1&#125;^m(&#123;\theta&#125;_0+&#123;\theta&#125;_1x^&#123;(i)&#125;-y^&#123;(i)&#125;)^2$<br>$j=0:\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_0&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)=\frac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)$<br>$j=1:\frac&#123;\partial&#125;&#123;\partial&#123;\theta&#125;_1&#125;J(&#123;\theta&#125;_0,&#123;\theta&#125;_1)=\frac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)x^&#123;(i)&#125;$<br>将得到的导数项代回式子，得到<br>repeat until convergence&#123;<br>$&#123;\theta&#125;_0:=&#123;\theta&#125;_0-\alpha\frac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)$<br>$&#123;\theta&#125;_1:=&#123;\theta&#125;_1-\alpha\frac&#123;1&#125;&#123;m&#125;\displaystyle\sum_&#123;i=1&#125;^m(h_&#123;\theta&#125;(x^&#123;(i)&#125;)-y^&#123;(i)&#125;)x^&#123;(i)&#125;$<br>&#125;<br>update $&#123;\theta&#125;_0$ and $&#123;\theta&#125;_1$ simultaneously 同时更新两个模型参数<br>对于线性回归模型，它的图形永远是呈现出伞形的，即它的代价函数是一个凸函数，它只有一个全局最优解，所以适用梯度下降算法。</p>
<h4 id="2-5-5、“Batch”-Gradient-Descent-批量梯度下降"><a href="#2-5-5、“Batch”-Gradient-Descent-批量梯度下降" class="headerlink" title="2-5-5、“Batch” Gradient Descent 批量梯度下降"></a>2-5-5、“Batch” Gradient Descent 批量梯度下降</h4><p>“Batch”：Each step of gradient descent uses all the training examples<br>在梯度下降的每一步中，我们都用到了所有的训练样本</p>
<h4 id="2-5-6、normal-equations-正规方程"><a href="#2-5-6、normal-equations-正规方程" class="headerlink" title="2-5-6、normal equations 正规方程"></a>2-5-6、normal equations 正规方程</h4><p>还有另一种求解代价函数最小值的数值解法，不需要梯度下降这种迭代算法。它可以在不需要多步梯度下降的情况下，也能解出代价函数的最小值。实际上，在数据量较大的情况下，梯度下降比正规方程要更适用一些。</p>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?d81ff4fc458aba1722010bdb220f0103";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
